---
title: "TP 3 : Support Vector Machine (SVM)"
author: ["Hachem Reda Riwa", "El Mazzouji Wahel"]
lang: fr
format:
  html:
    theme: flatly
    toc: false
    toc-location: left
    toc-depth: 3
    number-sections: false
    code-copy: true
    code-tools: true    
    code-fold: show 
    code-line-numbers: true
    smooth-scroll: true
    anchor-sections: true
    df-print: paged
    fig-align: center
    fig-cap-location: bottom
    tbl-cap-location: top
  pdf:
    documentclass: scrartcl
    toc: false
    number-sections: false
    geometry: "margin=2.2cm"
    fig-pos: "H"
execute:
  echo: false 
  warning: false
  cache: false
jupyter: python3
freeze: auto  
---

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

import os, sys
sys.path.insert(0, os.path.abspath("source")) 
from svm_source import *
from sklearn import svm
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from time import time

scaler = StandardScaler()

import warnings
warnings.filterwarnings("ignore")

plt.style.use('ggplot')
```
## Introduction

Les machines à vecteurs de support (SVM) constituent une méthode de classification supervisée largement utilisée pour leur capacité à séparer efficacement des classes, même dans des espaces de grande dimension. Leur principe repose sur la recherche d’un hyperplan séparateur maximisant la marge entre les classes, avec la possibilité d’utiliser des fonctions noyau afin de gérer des données non linéairement séparables.

Le but de ce TP est de mettre en pratique les SVM sur des données simulées et des jeux de données réels, à l’aide du package scikit-learn. Nous cherchons notamment à comprendre comment contrôler les paramètres influençant leur flexibilité, tels que les hyperparamètres et le choix du noyau.

## Mise en oeuvre sur la base de données Iris

Dans cette première section, les données seront partagées en un ensemble d’apprentissage (75 %) et un ensemble de test (25 %). Nous appliquerons un SVM avec noyau linéaire puis polynomial, afin de comparer leurs performances et l’impact du choix du noyau sur la frontière de décision.

### Question 1

Le noyau linéaire correspond au cas le plus simple des SVM, il consiste à chercher un hyperplan linéaire qui sépare au mieux les deux classes.
Nous avons restreint l’étude aux classes 1 et 2 du jeu de données Iris et entraîné un SVM à noyau linéaire. Le paramètre de régularisation C, qui contrôle le compromis entre largeur de marge et erreurs de classification, a été optimisé par validation croisée au moyen d’une recherche en grille.

```{python}
iris = datasets.load_iris()
X = iris.data
X = scaler.fit_transform(X)
y = iris.target
X = X[y != 0, :2]
y = y[y != 0]

# split train/test set
X , y = shuffle(X,y, random_state=42)
X_train, X_test, Y_train, Y_test = train_test_split(
    X, y, test_size=0.25, shuffle=False
)

# On convertit les labels en entiers
Y_train = Y_train.astype(int)
Y_test = Y_test.astype(int)


# fit the model
parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}
clf = SVC()
clf_linear = GridSearchCV(clf, parameters, n_jobs=-1)
clf_linear.fit(X_train, Y_train)

# Résultats
best_C = float(clf_linear.best_params_['C'])
print(f"Meilleur paramètre C : {best_C:.3f} (kernel = linéaire)")
print("Score entraînement :", round(clf_linear.score(X_train, Y_train),3))
print("Score test :", round(clf_linear.score(X_test, Y_test),3))
```

Le meilleur paramètre trouvé est C $\approx$ 8.31. Avec cette valeur, le modèle atteint une précision de 73% sur l'échantillon d'apprentissage et 72% sur l'échantillon de test.

### Question 2

Le noyau polynomial permet de projeter les données dans un espace de dimension supérieure en utilisant des combinaisons polynomiales des variables d’entrée, ce qui rend possible la séparation de classes non linéairement séparables dans l’espace initial. 

```{python}
Cs = list(np.logspace(-3, 3, 5))
gammas = 10. ** np.arange(1, 2)
degrees = np.r_[1, 2, 3]

parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}
clf = SVC()
clf_poly = GridSearchCV(clf, parameters, n_jobs=-1)
clf_poly.fit(X_train, Y_train)

# Résultats
best_params = clf_poly.best_params_
print(f"Meilleurs paramètres (poly) : C = {best_params['C']:.3f}, "
      f"degré = {best_params['degree']}, gamma = {best_params['gamma']}")

print(f"Score entraînement : {clf_poly.score(X_train, Y_train):.3f}")
print(f"Score test : {clf_poly.score(X_test, Y_test):.3f}")

# display your results using frontiere (svm_source.py)
def f_linear(xx):
    return clf_linear.predict(xx.reshape(1, -1))

def f_poly(xx):
    return clf_poly.predict(xx.reshape(1, -1))

plt.ion()
plt.figure(figsize=(15, 5))
plt.subplot(131)
plot_2d(X, y)
plt.title("Données Iris (classes 1 et 2)")

plt.subplot(132)
frontiere(f_linear, X, y)
plt.title("Frontière – noyau linéaire")

plt.subplot(133)
frontiere(f_poly, X, y)

plt.title("Frontière – noyau polynomial")
plt.tight_layout()
plt.draw()
```

Nous avons testé un SVM avec noyau polynomial en faisant varier les paramètres C, $\gamma$ et le degré du polynôme. Le meilleur modèle obtenu correspond en réalité à un polynôme de degré 1, équivalent à un noyau linéaire. 

Les scores obtenus ($\approx 72\%$) sont très proches de ceux du SVM linéaire, et la frontière de décision est la même. Cela montre que, dans ce cas, l’utilisation d’un noyau polynomial plus complexe n’apporte pas d’amélioration par rapport au noyau linéaire.

## SVM GUI

### Question 3

Dans cette partie nous avons utilisé le script `svm_gui.py`, pour étudier le paramètre de régularisation C.

On observe que lorsque C est grand, l’algorithme cherche à classer correctement tous les points, quitte à réduire la marge et à ajuster fortement la frontière de décision.

Lorsque C est petit, les erreurs sont davantage tolérées, la marge s’élargit et la frontière se déplace au profit de la classe majoritaire.

Nous avons généré un jeu de données fortement déséquilibré, composé de 90 points appartenant à une classe et seulement 10 à l’autre. Nous avons ensuite étudié l’effet du paramètre C avec un noyau linéaire.

![](images/C_1.png){fig-align="center" width=60%}

<div style="text-align: center;">
*Figure 1 – Visualisation de la frontière de décision pour C = 1*
</div>

![](images/C_0.01.png){fig-align="center" width=90%}

<div style="text-align: center;">
*Figure 2 – Visualisation de la frontière de décision pour C = 0.01*
</div>

On observe qu’avec $C = 1$, le SVM maintient une séparation correcte entre les deux classes, y compris pour la minorité, tout en élargissant légèrement la marge.

En revanche, avec $C = 0.01$, certains points verts franchissent l’hyperplan et sont mal classés, la frontière devient moins stricte et privilégie clairement la classe majoritaire.

Ainsi, la diminution de C élargit les marges et accentue le biais en faveur de la classe majoritaire, car le modèle tolère davantage d’erreurs sur la minorité.

Concrètement, cet effet peut être corrigé en ajustant la pondération des erreurs via le paramètre `class_weight` de `SVC`, afin de donner plus de poids à la classe minoritaire. Une autre approche consiste à utiliser une calibration des probabilités (option `probability=True`), permettant de rééquilibrer la décision du classifieur et d’améliorer la prise en compte des classes rares.

## Classification de visages

Dans cette section, nous appliquons les SVM à un problème de reconnaissance faciale en utilisant la base Labeled Faces in the Wild (LFW). L’objectif est de distinguer deux individus à partir d’images, en extrayant des caractéristiques puis en entraînant un classifieur SVM à noyau linéaire.

```{python}
#| include: false
import os, ssl, certifi, urllib.request
# 1) pointer vers le bundle certifi
os.environ["SSL_CERT_FILE"] = certifi.where()
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
# 2) forcer urllib à utiliser ce bundle
ctx = ssl.create_default_context(cafile=certifi.where())
opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=ctx))
urllib.request.install_opener(opener)

# Download the data and unzip; then load it as numpy arrays
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True, data_home="./scikit_learn_data")
# data_home='.'

# introspect the images arrays to find the shapes (for plotting)
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# the label to predict is the id of the person
target_names = lfw_people.target_names.tolist()

####################################################################
# Pick a pair to classify such as
names = ['Tony Blair', 'Colin Powell']
# names = ['Donald Rumsfeld', 'Colin Powell']

idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# plot a sample set of the data
plot_gallery(images, np.arange(12))
plt.show()

# Extract features

# features using only illuminations
X = (np.mean(images, axis=3)).reshape(n_samples, -1)

# # or compute features using colors (3 times more features)
# X = images.copy().reshape(n_samples, -1)

# Scale features
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

# Split data into a half training and half test set
# X_train, X_test, y_train, y_test, images_train, images_test = \
#    train_test_split(X, y, images, test_size=0.5, random_state=0)
# X_train, X_test, y_train, y_test = \
#    train_test_split(X, y, test_size=0.5, random_state=0)

indices = np.random.permutation(X.shape[0])
train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[
    train_idx, :, :, :], images[test_idx, :, :, :]
```


### Question 4

```{python}
np.random.seed(42)
print("=== SVM linéaire : influence de C ===")
t0 = time()

# fit a classifier (linear) and test all the Cs
Cs = 10. ** np.arange(-5, 6)
scores = []
for C in Cs:
    clf = SVC(kernel='linear', C=C)
    clf.fit(X_train, y_train)
    scores.append(clf.score(X_train, y_train))

ind = np.argmax(scores)
best_C, best_score = Cs[ind], scores[ind]
print(f"Meilleur C : {best_C:.1e}")
print(f"Score test optimal : {best_score:.3f}")

plt.figure()
plt.plot(Cs, scores)
plt.xlabel("Parametres de regularisation C")
plt.ylabel("Scores d'apprentissage")
plt.title("Influence de C sur la performance")
plt.xscale("log")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

print("Prédiction des noms des personnes sur l’échantillon de test")
t0 = time()

# predict labels for the X_test images with the best classifier
clf = SVC(kernel='linear', C=Cs[ind])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("Réalisé en %0.3fs" % (time() - t0))
# The chance level is the accuracy that will be reached when constantly predicting the majority class.

print(f"Niveau de hasard (majorité) : {max(np.mean(y), 1.-np.mean(y)):.3f}")
print(f"Taux de précision final sur le test : {clf.score(X_test, y_test):.3f}")
```

L’analyse de l’impact du paramètre de régularisation C montre que, pour des valeurs très faibles, le modèle est trop contraint et n’arrive pas à séparer correctement les classes, ce qui conduit à un phénomène de sous-apprentissage. Autour de $C \approx 1.0 \times 10^{-3}$, la performance atteint son optimum et se stabilise. En revanche, pour des valeurs trop grandes de C, le modèle tend à sur-apprendre sans gain significatif.

Le meilleur compromis est obtenu pour $C \approx 1.0 \times 10^{-3}$, avec un taux de précision d'environ 90 % sur l’échantillon de test, largement supérieur au niveau de hasard estimé à 62 %.


```{python}

# Qualitative evaluation of the predictions using matplotlib

prediction_titles = [title(y_pred[i], y_test[i], names)
                     for i in range(y_pred.shape[0])]

plot_gallery(images_test, prediction_titles)
plt.show()
```

La prédiction sur l’échantillon de test montre que le modèle distingue globalement bien les deux individus. Sur les images affichées, la majorité est correctement classée, bien que quelques erreurs subsistent, ce qui illustre les limites du classifieur.

```{python}

# Look at the coefficients
plt.figure()
plt.imshow(np.reshape(clf.coef_, (h, w)))
plt.colorbar()
plt.title("Coefficients appris par le SVM ")
plt.show()
```

La visualisation des coefficients appris met en évidence les zones les plus discriminantes des visages utilisées par le SVM. Cela confirme que le modèle capture bien les traits distinctifs permettant de différencier les individus, et illustre l’efficacité de cette approche pour la classification faciale.

### Question 5

Dans cette partie, nous avons évalué la robustesse du SVM en introduisant des variables de nuisance (300 dimensions supplémentaires générées aléatoirement).

```{python}
np.random.seed(42)
def run_svm_cv(_X, _y, label=""):
    _indices = np.random.permutation(_X.shape[0])
    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]
    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]
    _y_train, _y_test = _y[_train_idx], _y[_test_idx]

    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    _svr = svm.SVC()
    _clf_linear = GridSearchCV(_svr, _parameters)
    _clf_linear.fit(_X_train, _y_train)
    score_train = _clf_linear.score(_X_train, _y_train)
    score_test  = _clf_linear.score(_X_test, _y_test)

    print(f"[{label}] Score entraînement : {score_train:.3f} | Score test : {score_test:.3f}")

print("Score sans variable de nuisance")
run_svm_cv(X, y, label="Données originales")

print("Score avec variable de nuisance")
n_features = X.shape[1]
# On rajoute des variables de nuisances
sigma = 1
noise = sigma * np.random.randn(n_samples, 300, ) 
#with gaussian coefficients of std sigma
X_noisy = np.concatenate((X, noise), axis=1)
X_noisy = X_noisy[np.random.permutation(X.shape[0])]
run_svm_cv(X_noisy, y, label="Données bruitées")
```

Sans ajout de variables de nuisance, le SVM linéaire obtient un taux de précision parfait à l’entraînement (1.0) et une excellente performance en test ($\approx 0.91$).

En revanche, lorsque 300 variables aléatoires bruitées sont ajoutées, la performance en test chute fortement ($\approx 0.52$), alors que l’entraînement reste à 1.0.

Cela met en évidence que l’ajout de dimensions non informatives détériore la capacité de généralisation : le modèle sur-apprend en exploitant le bruit, ce qui entraîne une baisse nette de la précision sur de nouvelles données.

### Question 6

Pour atténuer l’effet des variables de nuisance, nous avons appliqué une réduction de dimension par analyse en composantes principales (ACP) sur les données bruitées, afin de ne conserver que les composantes expliquant le plus de variance.

```{python}
np.random.seed(42)
print("Score apres reduction de dimension")

n_components = 200  
pca = PCA(n_components=n_components).fit(X_noisy)
X_pca = pca.transform(X_noisy)
run_svm_cv(X_pca, y)
```

En pratique, pour de petites valeurs du nombre de composantes, l’algorithme ne converge pas (temps de calcul très long), ce qui nous a conduit à fixer le nombre de composantes à $n=200$. Avec 200 composantes principales, le modèle conserve un taux de précision parfait à l’entraînement, mais la performance en test reste limitée ($\approx 0.484$). Ce résultat montre que, bien que l’ACP réduise l’influence des dimensions inutiles, un trop grand nombre de composantes maintient encore une part importante de bruit. Le choix du nombre optimal de composantes est donc crucial pour améliorer la généralisation du modèle.

### Question 7

Dans le script fourni, la normalisation est effectuée avant la séparation entre apprentissage et test, comme l’illustre l’extrait de code ci-dessous.

```{python}
#| echo: true
#| eval: false
X = (np.mean(images, axis=3)).reshape(n_samples, -1)

X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

indices = np.random.permutation(X.shape[0])
train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[
    train_idx, :, :, :], images[test_idx, :, :, :]
```


Cette approche engendre une fuite d’information, dans la mesure où les statistiques issues du jeu de test (moyenne et variance) interviennent dans la transformation des données d’apprentissage. Une telle erreur introduit un biais et conduit à une surestimation de la performance du modèle, puisque l’indépendance du jeu de test n’est plus respectée. La procédure correcte consiste à ajuster la normalisation uniquement sur l’échantillon d’apprentissage, puis à appliquer la transformation ainsi obtenue aux données de test, garantissant ainsi une évaluation fidèle de la capacité de généralisation du classifieur.

## Conclusion

Ce TP nous a permis de mettre en pratique les machines à vecteurs de support sur différents jeux de données. Nous avons observé l’influence du paramètre de régularisation C, l’impact du choix du noyau, ainsi que les limites liées au bruit et au déséquilibre des classes. Nous avons également vu comment des biais dans le prétraitement peuvent fausser l’évaluation des performances. Enfin, l’étude sur les visages a montré l’efficacité des SVM pour la classification d’images, tout en soulignant la nécessité d’un bon choix de paramètres et d’une réduction de dimension adaptée.