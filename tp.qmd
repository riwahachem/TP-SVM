---
title: "TP 3 : Support Vector Machine (SVM)"
author: ["Hachem Reda Riwa", "El Mazzouji Wahel"]
lang: fr
format:
  html:
    theme: flatly
    toc: true
    toc-location: left
    toc-depth: 3
    number-sections: false
    code-copy: true
    code-tools: true    
    code-fold: show 
    code-line-numbers: true
    smooth-scroll: true
    anchor-sections: true
    df-print: paged
    fig-align: center
    fig-cap-location: bottom
    tbl-cap-location: top
  pdf:
    documentclass: scrartcl
    toc: true
    number-sections: false
    geometry: "margin=2.2cm"
    fig-pos: "H"
execute:
  echo: false 
  warning: false
  cache: true
jupyter: python3
freeze: auto  
---

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

import os, sys
sys.path.insert(0, os.path.abspath("source")) 
from svm_source import *
from sklearn import svm
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from time import time

scaler = StandardScaler()

import warnings
warnings.filterwarnings("ignore")

plt.style.use('ggplot')
```

### Question 1

```{python}
iris = datasets.load_iris()
X = iris.data
X = scaler.fit_transform(X)
y = iris.target
X = X[y != 0, :2]
y = y[y != 0]

# split train/test set
X_train, X_test, Y_train, Y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# On convertit les labels en entiers
Y_train = Y_train.astype(int)
Y_test = Y_test.astype(int)


# fit the model
parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}
clf = SVC()
clf_linear = GridSearchCV(clf, parameters, n_jobs=-1)
clf_linear.fit(X_train, Y_train)

# Résultats
best_C = float(clf_linear.best_params_['C'])
print(f"Meilleur paramètre C : {best_C:.3f} (kernel = linéaire)")
print("Score entraînement :", round(clf_linear.score(X_train, Y_train),3))
print("Score test :", round(clf_linear.score(X_test, Y_test),3))
```

Avec un SVM à noyau linéaire entraîné sur les classes 1 et 2 du jeu de données Iris, le meilleur paramètre trouvé est C ≈ 0.841.
Le modèle atteint un score d’entraînement de 74% et un score de test de 68%.

### Question 2

```{python}
Cs = list(np.logspace(-3, 3, 5))
gammas = 10. ** np.arange(1, 2)
degrees = np.r_[1, 2, 3]

parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}
clf = SVC()
clf_poly = GridSearchCV(clf, parameters, n_jobs=-1)
clf_poly.fit(X_train, Y_train)

# Résultats
best_params = clf_poly.best_params_
print(f"Meilleurs paramètres (poly) : C = {best_params['C']:.3f}, "
      f"degré = {best_params['degree']}, gamma = {best_params['gamma']}")

print(f"Score entraînement : {clf_poly.score(X_train, Y_train):.3f}")
print(f"Score test : {clf_poly.score(X_test, Y_test):.3f}")

# display your results using frontiere (svm_source.py)
def f_linear(xx):
    return clf_linear.predict(xx.reshape(1, -1))

def f_poly(xx):
    return clf_poly.predict(xx.reshape(1, -1))

plt.ion()
plt.figure(figsize=(15, 5))
plt.subplot(131)
plot_2d(X, y)
plt.title("Données Iris (classes 1 et 2)")

plt.subplot(132)
frontiere(f_linear, X, y)
plt.title("Frontière – noyau linéaire")

plt.subplot(133)
frontiere(f_poly, X, y)

plt.title("Frontière – noyau polynomial")
plt.tight_layout()
plt.draw()
```

Un SVM avec noyau polynomial a été testé en faisant varier les paramètres C, gamma et le degré du polynôme.

Le meilleur modèle polynomial obtenu correspond en réalité à un polynôme de degré 1, ce qui revient à un noyau linéaire.
On observe que les scores sont identiques à ceux obtenus avec le SVM linéaire (74% sur l’entraînement et 68% sur le test).
La frontière de décision est donc la même qu’avec le noyau linéaire, ce qui montre que, dans ce cas, l’utilisation d’un noyau polynomial n’apporte pas d’amélioration.

## SVM GUI

### Question 3

Dans le script `svm_gui.py`, le paramètre de régularisation C contrôle le compromis entre largeur de la marge et pénalisation des erreurs de classification.

Lorsque C est grand, l’algorithme cherche à classer correctement tous les points, quitte à réduire la marge et à ajuster fortement la frontière de décision.

Lorsque C est petit, les erreurs sont davantage tolérées, la marge s’élargit et la frontière se déplace au profit de la classe majoritaire.

Nous avons généré un jeu de données fortement déséquilibré, composé de 90 points appartenant à une classe et seulement 10 à l’autre. Nous avons ensuite étudié l’effet du paramètre C avec un noyau linéaire.

![](images/C_1.png){fig-align="center" width=60%}

<div style="text-align: center;">
*Figure 1 – Visualisation de la frontière de décision pour C = 1*
</div>

![](images/C_0.01.png){fig-align="center" width=90%}

<div style="text-align: center;">
*Figure 2 – Visualisation de la frontière de décision pour C = 0.01*
</div>

On observe qu’avec C = 1, la SVM maintient une séparation correcte entre les deux classes, y compris pour la minorité, tout en élargissant légèrement la marge.

En revanche, avec C = 0.01, certains points verts franchissent l’hyperplan et sont mal classés, la frontière devient moins stricte et privilégie clairement la classe majoritaire.

Ainsi, la diminution de C élargit les marges et accentue le biais en faveur de la classe majoritaire, car le modèle tolère davantage d’erreurs sur la minorité.

## Classification de visages

```{python}
#| include: false
import os, ssl, certifi, urllib.request
# 1) pointer vers le bundle certifi
os.environ["SSL_CERT_FILE"] = certifi.where()
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
# 2) forcer urllib à utiliser ce bundle
ctx = ssl.create_default_context(cafile=certifi.where())
opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=ctx))
urllib.request.install_opener(opener)

# Download the data and unzip; then load it as numpy arrays
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True, data_home="./scikit_learn_data")
# data_home='.'

# introspect the images arrays to find the shapes (for plotting)
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# the label to predict is the id of the person
target_names = lfw_people.target_names.tolist()

####################################################################
# Pick a pair to classify such as
names = ['Tony Blair', 'Colin Powell']
# names = ['Donald Rumsfeld', 'Colin Powell']

idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# plot a sample set of the data
plot_gallery(images, np.arange(12))
plt.show()

# Extract features

# features using only illuminations
X = (np.mean(images, axis=3)).reshape(n_samples, -1)

# # or compute features using colors (3 times more features)
# X = images.copy().reshape(n_samples, -1)

# Scale features
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

# Split data into a half training and half test set
# X_train, X_test, y_train, y_test, images_train, images_test = \
#    train_test_split(X, y, images, test_size=0.5, random_state=0)
# X_train, X_test, y_train, y_test = \
#    train_test_split(X, y, test_size=0.5, random_state=0)

indices = np.random.permutation(X.shape[0])
train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[
    train_idx, :, :, :], images[test_idx, :, :, :]
```


### Question 4

```{python}
print("=== SVM linéaire : influence de C ===")
t0 = time()

# fit a classifier (linear) and test all the Cs
Cs = 10. ** np.arange(-5, 6)
scores = []
for C in Cs:
    clf = SVC(kernel='linear', C=C)
    clf.fit(X_train, y_train)
    scores.append(clf.score(X_train, y_train))

ind = np.argmax(scores)
best_C, best_score = Cs[ind], scores[ind]
print(f"Meilleur C : {best_C:.1e}")
print(f"Score test optimal : {best_score:.3f}")

plt.figure()
plt.plot(Cs, scores)
plt.xlabel("Parametres de regularisation C")
plt.ylabel("Scores d'apprentissage")
plt.title("Influence de C sur la performance")
plt.xscale("log")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

print("Prédiction des noms des personnes sur l’échantillon de test")
t0 = time()

# predict labels for the X_test images with the best classifier
clf = SVC(kernel='linear', C=Cs[ind])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("Réalisé en %0.3fs" % (time() - t0))
# The chance level is the accuracy that will be reached when constantly predicting the majority class.

print(f"Niveau de hasard (majorité) : {max(np.mean(y), 1.-np.mean(y)):.3f}")
print(f"Taux de précision final sur le test : {clf.score(X_test, y_test):.3f}")
```

Lorsque C est très petit, le modèle est trop régularisé et n’arrive pas à bien séparer les classes (sous-apprentissage).
Autour de C ≈ $1.0 \times 10^{-3}$ , la performance devient optimale et se stabilise.
Pour des valeurs trop grandes de C, le modèle risque de sur-apprendre sans gain significatif.
Le meilleur compromis est obtenu avec C ≈ $1.0 \times 10^{-3}$, qui donne un taux de précision de `r clf.score(X_test, y_test)*100:.1f`% sur le test, bien supérieur au hasard (`r max(np.mean(y), 1.-np.mean(y)):.1f`%).


```{python}

# Qualitative evaluation of the predictions using matplotlib

prediction_titles = [title(y_pred[i], y_test[i], names)
                     for i in range(y_pred.shape[0])]

plot_gallery(images_test, prediction_titles)
plt.show()
```

La prédiction sur l’échantillon de test montre que le modèle arrive à bien distinguer les deux individus.

```{python}

# Look at the coefficients
plt.figure()
plt.imshow(np.reshape(clf.coef_, (h, w)))
plt.colorbar()
plt.title("Coefficients appris par le SVM ")
plt.show()
```

La visualisation des coefficients montre que le SVM capte les traits distinctifs des visages, confirmant l’efficacité de cette approche pour la classification faciale.

### Question 5

```{python}
def run_svm_cv(_X, _y, label=""):
    _indices = np.random.permutation(_X.shape[0])
    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]
    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]
    _y_train, _y_test = _y[_train_idx], _y[_test_idx]

    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    _svr = svm.SVC()
    _clf_linear = GridSearchCV(_svr, _parameters)
    _clf_linear.fit(_X_train, _y_train)
    score_train = _clf_linear.score(_X_train, _y_train)
    score_test  = _clf_linear.score(_X_test, _y_test)

    print(f"[{label}] Score entraînement : {score_train:.3f} | Score test : {score_test:.3f}")

print("Score sans variable de nuisance")
run_svm_cv(X, y, label="Données originales")

print("Score avec variable de nuisance")
n_features = X.shape[1]
# On rajoute des variables de nuisances
sigma = 1
noise = sigma * np.random.randn(n_samples, 300, ) 
#with gaussian coefficients of std sigma
X_noisy = np.concatenate((X, noise), axis=1)
X_noisy = X_noisy[np.random.permutation(X.shape[0])]
run_svm_cv(X_noisy, y, label="Données bruitées")
```

Sans ajout de variables de nuisance, le SVM linéaire obtient un taux de précision parfait à l’entraînement (1.0) et une très bonne performance en test :`r {score_test:.3f}`.
En revanche, lorsque l’on ajoute 300 variables bruitées, la performance en test chute fortement (`r {score_test_noisy:.3f}`), tandis que l’entraînement reste à 1.0.

Cela montre que l’ajout de dimensions non informatives dégrade la capacité de généralisation : le classifieur se retrouve à sur-apprendre sur des données bruitées, ce qui entraîne une baisse nette de la précision sur de nouvelles données.

### Question 6

```{python}
print("Score apres reduction de dimension")

n_components = 200  
pca = PCA(n_components=n_components).fit(X_noisy)
X_pca = pca.transform(X_noisy)
run_svm_cv(X_pca, y)
```

Avec la réduction de dimension par PCA, le modèle conserve un taux de précision parfait sur l’entraînement mais la performance en test reste faible (≈ `r {score_test_pca:.3f}` pour 200 composantes). Cela s’explique par le fait qu’un trop grand nombre de composantes conserve encore beaucoup de bruit.

### Question 7

Dans le script, la normalisation est faite avant de séparer les données en apprentissage et test, par les lignes :
`X -= np.mean(X, axis=0)` et `X /= np.std(X, axis=0)`.
Cela pose problème car les statistiques du test (moyenne, variance) sont utilisées pour transformer les données d’entraînement.
La bonne pratique consiste à calculer la normalisation uniquement sur l’échantillon d’apprentissage, puis à appliquer cette transformation au test. Sinon, les performances rapportées ne reflètent pas la capacité réelle de généralisation du modèle.